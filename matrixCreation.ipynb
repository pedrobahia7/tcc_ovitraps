{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\UFMG\\\\Periodos\\\\Periodo_XIV\\\\TCC\\\\Projeto\\\\CÃ³digos\\\\tcc_ovitraps\\\\utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import numpy as np\n",
    "import importlib \n",
    "from tqdm import tqdm\n",
    "\n",
    "importlib.reload(utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/final_data.csv',parse_dates=['dtcol'])\n",
    "valid_samples = data[['nplaca','novos','latitude','longitude','narmad','ano','semepi','dtcol']].drop_duplicates().dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 3 # number of lags to consider of all traps\n",
    "n_traps = 3 # including the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment \n",
    "TODO: move to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traps with the same lat_long  narmad\n",
      "902016    305\n",
      "902017    291\n",
      "908062    288\n",
      "902022    283\n",
      "907076    260\n",
      "907080    258\n",
      "907111    221\n",
      "902167    217\n",
      "907106    191\n",
      "902169    160\n",
      "908095    128\n",
      "907105    109\n",
      "908094    100\n",
      "907110     79\n",
      "902168     78\n",
      "907081     42\n",
      "907077     39\n",
      "902021     16\n",
      "908105     15\n",
      "908103     15\n",
      "902015     10\n",
      "908063      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dates with traps with the same lat_long  dtcol\n",
      "2020-04-06    8\n",
      "2015-12-07    7\n",
      "2018-11-19    7\n",
      "2018-10-08    7\n",
      "2022-02-14    6\n",
      "2023-12-04    6\n",
      "2023-01-16    6\n",
      "2019-12-02    6\n",
      "2019-01-14    6\n",
      "2019-04-08    6\n",
      "Name: novos, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "groups = valid_samples.groupby(['latitude','longitude'])\n",
    "rows_to_update = []\n",
    "\n",
    "for name, group in groups:\n",
    "    if group['narmad'].nunique() > 1:  # More than one unique value in C\n",
    "        rows_to_update.extend(group.index.tolist())  # Add row indices to the list\n",
    "bad_data = data.loc[rows_to_update]\n",
    "\n",
    "print('Traps with the same lat_long ',bad_data['narmad'].value_counts())\n",
    "print('\\nDates with traps with the same lat_long ',bad_data.groupby(['dtcol'])['novos'].nunique().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narmad\n",
      "1    1760\n",
      "2       5\n",
      "3       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# This cell creates a new dataframe changing the number of the traps that are in the same position\n",
    "# TODO move to data exploration of preprocessing\n",
    "same_position_armad = valid_samples.groupby(['latitude','longitude'])['narmad'].nunique()\n",
    "print(same_position_armad.value_counts()) #number of traps in the same position\n",
    "\n",
    "#data['narmad'] = data.groupby(['latitude','longitude'])['narmad'].transform('first') #group by position and take the first trap number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce a small value on this traps to differentiate them\n",
    "for trap in bad_data['narmad'].unique():\n",
    "    valid_samples.loc[valid_samples['narmad'] == trap, 'latitude'] += np.random.rand()*0.00000001\n",
    "    valid_samples.loc[valid_samples['narmad'] == trap, 'longitude'] += np.random.rand()*0.00000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1570878it [06:45, 3876.51it/s]\n"
     ]
    }
   ],
   "source": [
    "position_matrix = valid_samples[['latitude','longitude','narmad']].drop_duplicates().dropna().reset_index(drop=True)\n",
    "\n",
    "try: \n",
    "    distance_matrix = pd.read_csv('./results/distance_matrix.csv',index_col=0)\n",
    "    distance_matrix.columns = distance_matrix.columns.astype(float)\n",
    "    distance_matrix.set_index(distance_matrix.columns,inplace=True)\n",
    "\n",
    "\n",
    "except:\n",
    "    distance_matrix = utils.create_distance_matrix(position_matrix)\n",
    "    distance_matrix.to_csv('./results/distance_matrix.csv')\n",
    "\n",
    "distance_matrix_np = distance_matrix.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week trap matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_trap_df = valid_samples.pivot(index=['ano','semepi'],columns='narmad',values='novos')\n",
    "new_index = pd.MultiIndex.from_product([week_trap_df.index.levels[0], range(101,153)]) # introduce weeks 51 and 52\n",
    "new_index = new_index[(new_index <= week_trap_df.iloc[-1].name) & (new_index >= week_trap_df.iloc[0].name)] #remove indexes that are greater than the last sample or smaller than the first one\n",
    "week_trap_df = week_trap_df.reindex(new_index) # [week,trap] - > novos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lagged_df = []\n",
    "for i in range(1,2*lags+1):\n",
    "    list_lagged_df.append(week_trap_df.shift(i).values)\n",
    "lagged_matrix = np.stack(list_lagged_df, axis=0) # numpy array with the lagged values. [lag x week x trap] -> novos. Obs.: consider 2*lags due to the biweekly sampling rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Count Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with 1 valid values:  0\n",
      "Number of samples with 2 valid values:  0\n",
      "Number of samples with 3 valid values:  742782\n",
      "Number of samples with 4 valid values:  266579\n",
      "Number of samples with 5 valid values:  28622\n",
      "Number of samples with 6 valid values:  157019\n"
     ]
    }
   ],
   "source": [
    "nan_count_matrix = np.sum(np.isnan(lagged_matrix), axis=0) # [week x trap] -> number of nans in the lagged matrix\n",
    "\n",
    "for i in range(2*lags):\n",
    "    print(\"Number of samples with\",i+1,\"valid values: \",np.sum(nan_count_matrix==i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = valid_samples[['ano','semepi','nplaca','dtcol','novos','narmad','latitude','longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged days matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = valid_samples.pivot(index=['ano','semepi'],columns='narmad',values='dtcol')\n",
    "new_index = pd.MultiIndex.from_product([day_df.index.levels[0], range(101,153)]) # introduce weeks 51 and 52\n",
    "new_index = new_index[(new_index <= day_df.iloc[-1].name) & (new_index >= day_df.iloc[0].name)] #remove indexes that are greater than the last sample or smaller than the first one\n",
    "day_df = day_df.reindex(new_index) # [week,trap] - > dtcol\n",
    "day_df = day_df.map(lambda x: x.toordinal() if pd.notnull(x) else np.nan) # convert to ordinal so we can calculate the difference between two dates\n",
    "day_df_np = day_df.to_numpy()\n",
    "list_lagged_days = []\n",
    "\n",
    "for i in range(1,2*lags+1):\n",
    "    list_lagged_days.append(day_df.shift(i).values)\n",
    "lagged_days = np.stack(list_lagged_days, axis=0) # numpy array with the lagged values. [lag x week x trap] -> ordinal daus. Obs.: consider 2*lags due to the biweekly sampling rate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trap_index_dict = {trap: index for index,trap in enumerate(distance_matrix.columns)}                                           # trap: index \n",
    "yearweek_index_dict = {(year,week): index for index,(year,week) in enumerate(week_trap_df.index)}                           # (year,week): index\n",
    "nplaca_week_dict = {nplaca: (year, week) for nplaca,week,year in zip(info_df['nplaca'],info_df['semepi'],info_df['ano'])}   # nplaca: (year,week)\n",
    "nplaca_index_dict = {nplaca: yearweek_index_dict[(year, week)] for nplaca,week,year in zip(info_df['nplaca'],info_df['semepi'],info_df['ano'])}   # nplaca: week index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_placas = []\n",
    "list_final_samples = []\n",
    "for original_trap in distance_matrix.columns:\n",
    "    matching_placas = info_df[info_df['narmad'] == original_trap]['nplaca']\n",
    "    original_trap_index = trap_index_dict[original_trap]\n",
    "    sorted_distance_indexes = np.argsort(distance_matrix_np[original_trap_index])\n",
    "    #assert sorted_distance_indexes[0] == trap_index, 'trap is not the closest to itself'\n",
    "    for placa in matching_placas: \n",
    "        add_row = [placa]\n",
    "        order_index = 0\n",
    "        list_placas.append(placa)\n",
    "        week_index = nplaca_index_dict[placa]\n",
    "\n",
    "        # remove samples of original traps that doesn't have enough autoregressive samples \n",
    "        if nan_count_matrix[week_index,original_trap_index] > lags: \n",
    "            continue\n",
    "\n",
    "        for trap in range(n_traps): #loop to deal with the n_traps closest to the original trap\n",
    "            trap_index = sorted_distance_indexes[order_index]\n",
    "\n",
    "            while nan_count_matrix[week_index,trap_index] > lags: #  avoid traps that doesn't have enough autoregressive samples\n",
    "                order_index += 1\n",
    "                trap_index = sorted_distance_indexes[order_index]\n",
    "                #if order_index > 50: #avoid arbitrarily distant traps\n",
    "                    #break\n",
    "            \n",
    "            lagged_samples = lagged_matrix[:,week_index,trap_index] # [lag x week x trap] -> novos\n",
    "\n",
    "            [add_row.append(i) for i in lagged_samples[~np.isnan(lagged_samples)]] # add lagged novos\n",
    "            add_row.append(distance_matrix_np[original_trap_index,trap_index]) # add distance\n",
    "            \n",
    "            #subtract lagged days from orignal sample day [lag x week x trap] -> ordinal days\n",
    "            lagged_samples_days = lagged_days[:,week_index,trap_index]\n",
    "            days_diff = day_df_np[week_index,original_trap_index] - lagged_samples_days[~np.isnan(lagged_samples_days)] \n",
    "            [add_row.append(i) for i in days_diff] # add lagged days\n",
    "            if np.isnan(days_diff).any():\n",
    "                print('nan')\n",
    "       \n",
    "            order_index += 1\n",
    "        list_final_samples.append(add_row)\n",
    "\n",
    "\n",
    "assert len(list_placas) == valid_samples.shape[0], 'invalid number of placas'\n",
    "assert len(list_placas) == len(set(list_placas)), 'duplicated placas'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(list_final_samples)\n",
    "\n",
    "columns_names = ['nplaca']\n",
    "for j in range(n_traps): \n",
    "    for i in range(1,lags+1):\n",
    "        columns_names.extend(['trap'+str(j)+'_lag'+str(i)])\n",
    "    columns_names.extend(['distance'+str(j)])\n",
    "    for i in range(1,lags+1):\n",
    "        columns_names.extend(['days'+str(j)+'_lag'+str(i)])\n",
    "\n",
    "\n",
    "\n",
    "final_df.columns = columns_names  \n",
    "final_df = pd.merge(data[['nplaca','novos']],final_df,how='inner',on='nplaca')\n",
    "final_df.set_index('nplaca',inplace=True)\n",
    "final_df.to_csv(f'./results/final_df_lag{lags}_ntraps{n_traps}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ovitraps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
