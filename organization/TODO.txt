New:


Treinar NN e ver porcentagem de armadilhas maiores que "600"

Comparar com Naïve e modelar erro do naive

Music Informatics - método para identificar reta e ponto onde ela começa/

Informação Mútua e correlação de Spearman para definir correlações

Granger causality to add new traps and external variables

Classificador para definir qual das distribuilções seguir

Comparar análise bayesiana com inteiramente computacional

Rede baseada em grafosd

pacotes de ajuste de distriuição automatica

log da variavel de saída -> histograma para verificar se cauda pesada está ausente (mesma análise para boxcox)

cluster espacial como variável (análise espacia!) -> talvez introdução de variáveis exógenas tirem a dependcência geográfica (modelar e analisar resíduos)

Modelos ARIMA, SARIMA, RF, SVM(com kernels diferentes)

Modelagem de baixa frgequência para verificar tendência e comparar com armadilhas vizinhas. Talvez elas tenham curvas parecidas, apenas multiplicadas por um fator de escala
    
Introduzir informação de sazonalidade na modelagem

aprendizado hebbiano para lidar com dados espaço temporais w_hebb = x_0y_0                  



Ler: 
    Artigos:
        https://homepages.dcc.ufmg.br/~olmo/mypapers/p1319-vazdemelo.pdf (log logistic)
        https://epubs.siam.org/doi/10.1137/070710111 (Renato)
        https://arxiv.org/abs/2406.16232 (Renato)



PROBLEMS:
Percentage of zeros in exponential_renato is doubled
loss function logistic
adapt input_3d to logistic and linear regressor
3D dimension: lat, long, week, year consatants to original trap
dias negativos na matrix final 
january data is dropped in final matrix with lag 3 
diferenças dos dias de armação e coleta -> se forem muito grandes, adicionar como variável da NN? Valores absurdos. Filtrar?
Tratamento de NaNs na Latitude/Longitude (perguntar dilermando )
Substituir logistic and linear regression por sklearn
Semanas pares e ímpares número de Nans no week 2023
retirar pareto individual do mapa
Loss function regresion >>> Loss function classification -> terrible network (everything is 0 or 1). Create a second network only with non null values. after classification or before it? 
substituir dia por semana epid

introduzir double poisson na função de custo 

Renato:

Plots series temporais para introdução de variavel sazonal
categorias como forma de predição (alto número de zeros) - mapa e tabela em treated_df_exploration
Plots de mediana de lags x atual
Diferença de dias por lag
diferença dtcol 
difernça entre samples
dados de janeiro
!!!rede exponencial separada!!!
alfa < 1: truncar com relu
double poisson: aprendendo ferramenta para incorporar 

vairaveis exogenas
log da variavel de saída 
pacotes de ajuste de distriuição automatica
cluster regional
Modelos ARIMA, SARIMA, RF, SVM, catboost
aumentar número de épocas para treinamento
comparar datasets de treino e teste para ver se eles seguem mesma distribuição
Try to replicate double descentent in my data (ELM(increase p and plot), MLP) increase size and epochs
plot nsamples by neigh and lags

LOGISTICA:
spline(with different degrees) remove semepi2, sinsemepi and mesepid
Analisar quantos ovos em média tem as armadilhas que erramos
Verificar na literatura referenciuas para erros de classificação
Naive para classificador booleano
add number of samples as a criteria
analise de coeficientes dos dias
Usar p-valor no lugar de stepwise
Plot y vs naive


version:
1: lags, days, lat long
2: mesepid
3: perc_zero
4: perc_one
5: semedpid, semepid2, sin_semepid
6: 100 truncado

ScoreG in half of the dataset and use the other half for training and test
truncatey y traps in 100 and create linear regression 

ELM (train error vs numebr of hidden layers close to number of training samples)
outras variaveis
seleção de entradas via correlation


C:\Users\UFMG\Periodos\Periodo_XIV\TCC\Projeto\Códigos\tcc_ovitraps\utils\NN_building.py:73: PerformanceWarning: DataFrame is highly fragmented.  
This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  data['semepi2'] = data['semepi']**2
C:\Users\UFMG\Periodos\Periodo_XIV\TCC\Projeto\Códigos\tcc_ovitraps\utils\NN_building.py:74: PerformanceWarning: DataFrame is highly fragmented.  
This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  data['sin_semepi'] = np.sin(np.pi*data['semepi']/max(data['semepi']))

