{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "def scale_column(x_train:pd.DataFrame, x_test:pd.DataFrame, column:list):\n",
    "    \"\"\"\n",
    "    Move to utils_NN.py\n",
    "    Scales the same nature columns of x_train and x_test using the MinMaxScaler. The reference column is the one with the maximum value.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    max_value = x_train[column].max().max()\n",
    "    x_train[column] = x_train[column]/max_value\n",
    "    x_test[column]  = x_test[column]/max_value\n",
    "    return x_train, x_test, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_type =  'classifier' # 'classifier' or 'regressor'\n",
    "use_trap_info = True\n",
    "ntraps = 3\n",
    "lags = 3\n",
    "random_split = True\n",
    "test_size = 0.2\n",
    "scale = True\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import and preprocessing\n",
    "data = pd.read_csv(f'./results/final_df_lag{lags}_ntraps{ntraps}.csv')\n",
    "n = data.shape[0]\n",
    "nplaca_index = data['nplaca']\n",
    "data.drop(columns=['nplaca','distance0'], inplace=True) # drop distance0 because it is always zero\n",
    "ovos_flag = data['novos'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# divide columns into groups\n",
    "days_columns = [f'days{i}_lag{j}' for i in range(ntraps) for j in range(1, lags+1)]\n",
    "distance_columns = [f'distance{i}' for i in range(1,ntraps)]\n",
    "eggs_columns = [f'trap{i}_lag{j}' for i in range(ntraps) for j in range(1, lags+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of x and y\n",
    "if model_type == 'classifier':\n",
    "    y = ovos_flag\n",
    "else:\n",
    "    y = data['novos']\n",
    "if use_trap_info:\n",
    "    x = data.drop(columns=['novos'])\n",
    "else:\n",
    "    drop_cols = ['novos'] + days_columns + distance_columns\n",
    "    x = data.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = 1 - test_size\n",
    "\n",
    "if random_split:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.drop(columns=['novos']), y, test_size=test_size, random_state=42, stratify=ovos_flag)\n",
    "else:\n",
    "    y_train = y.iloc[:int(n*train_size)]\n",
    "    y_test = y.iloc[int(n*train_size):]\n",
    "    x_train = x.iloc[:int(n*train_size)]\n",
    "    x_test = x.iloc[int(n*train_size):]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "x_train, x_test, max_eggs = scale_column(x_train, x_test, eggs_columns)\n",
    "if use_trap_info:\n",
    "    x_train, x_test, max_distance = scale_column(x_train, x_test, distance_columns)\n",
    "    x_train, x_test, max_days = scale_column(x_train, x_test, days_columns)\n",
    "\n",
    "\n",
    "if model_type != 'classifier':\n",
    "    y_train = y_train/max_eggs\n",
    "    y_test = y_test/max_eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6884\\450902588.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = torch.tensor(features, dtype=torch.float32)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6884\\450902588.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets = torch.tensor(targets, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# transform to tensors\n",
    "xtrain = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "ytrain = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "xtest = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "ytest = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = CustomDataset(xtrain, ytrain)\n",
    "test_dataset = CustomDataset(xtest, ytest)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=random_split)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=random_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network structure\n",
    "if use_trap_info:\n",
    "    model_input = lags*ntraps + ntraps-1 + ntraps*lags # sum  of eggs, distances minus one and days\n",
    "else:\n",
    "    model_input = lags*ntraps\n",
    "    \n",
    "if model_type == 'classifier':\n",
    "    model_output = 2\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.structure = nn.Sequential(\n",
    "            nn.Linear(model_input, 20),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(10, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5, model_output)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.structure(x)\n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test loops\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,batch_size):\n",
    "    size = xtrain.shape[0]\n",
    "    model.train()\n",
    "    for batch, (xtest, ytest) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xtest)\n",
    "        loss = loss_fn(pred, ytest)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(xtest)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Metrics: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.694440  [   64/267656]\n",
      "loss: 0.698057  [ 6464/267656]\n",
      "loss: 0.696385  [12864/267656]\n",
      "loss: 0.693171  [19264/267656]\n",
      "loss: 0.685402  [25664/267656]\n",
      "loss: 0.688050  [32064/267656]\n",
      "loss: 0.692805  [38464/267656]\n",
      "loss: 0.679905  [44864/267656]\n",
      "loss: 0.677932  [51264/267656]\n",
      "loss: 0.676404  [57664/267656]\n",
      "loss: 0.685993  [64064/267656]\n",
      "loss: 0.673894  [70464/267656]\n",
      "loss: 0.682259  [76864/267656]\n",
      "loss: 0.675038  [83264/267656]\n",
      "loss: 0.696304  [89664/267656]\n",
      "loss: 0.659471  [96064/267656]\n",
      "loss: 0.666422  [102464/267656]\n",
      "loss: 0.700801  [108864/267656]\n",
      "loss: 0.705110  [115264/267656]\n",
      "loss: 0.664731  [121664/267656]\n",
      "loss: 0.672869  [128064/267656]\n",
      "loss: 0.672604  [134464/267656]\n",
      "loss: 0.689705  [140864/267656]\n",
      "loss: 0.676588  [147264/267656]\n",
      "loss: 0.694563  [153664/267656]\n",
      "loss: 0.685538  [160064/267656]\n",
      "loss: 0.690279  [166464/267656]\n",
      "loss: 0.699673  [172864/267656]\n",
      "loss: 0.657429  [179264/267656]\n",
      "loss: 0.676139  [185664/267656]\n",
      "loss: 0.642855  [192064/267656]\n",
      "loss: 0.709734  [198464/267656]\n",
      "loss: 0.695359  [204864/267656]\n",
      "loss: 0.656802  [211264/267656]\n",
      "loss: 0.676040  [217664/267656]\n",
      "loss: 0.709729  [224064/267656]\n",
      "loss: 0.680888  [230464/267656]\n",
      "loss: 0.719774  [236864/267656]\n",
      "loss: 0.695373  [243264/267656]\n",
      "loss: 0.666346  [249664/267656]\n",
      "loss: 0.685696  [256064/267656]\n",
      "loss: 0.646837  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680470 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.646754  [   64/267656]\n",
      "loss: 0.631660  [ 6464/267656]\n",
      "loss: 0.695751  [12864/267656]\n",
      "loss: 0.695939  [19264/267656]\n",
      "loss: 0.670840  [25664/267656]\n",
      "loss: 0.670717  [32064/267656]\n",
      "loss: 0.675774  [38464/267656]\n",
      "loss: 0.691106  [44864/267656]\n",
      "loss: 0.701201  [51264/267656]\n",
      "loss: 0.690881  [57664/267656]\n",
      "loss: 0.680892  [64064/267656]\n",
      "loss: 0.655856  [70464/267656]\n",
      "loss: 0.700990  [76864/267656]\n",
      "loss: 0.640990  [83264/267656]\n",
      "loss: 0.691020  [89664/267656]\n",
      "loss: 0.650651  [96064/267656]\n",
      "loss: 0.665914  [102464/267656]\n",
      "loss: 0.695997  [108864/267656]\n",
      "loss: 0.680890  [115264/267656]\n",
      "loss: 0.690900  [121664/267656]\n",
      "loss: 0.711002  [128064/267656]\n",
      "loss: 0.641244  [134464/267656]\n",
      "loss: 0.690748  [140864/267656]\n",
      "loss: 0.670983  [147264/267656]\n",
      "loss: 0.695921  [153664/267656]\n",
      "loss: 0.675896  [160064/267656]\n",
      "loss: 0.661082  [166464/267656]\n",
      "loss: 0.680888  [172864/267656]\n",
      "loss: 0.680893  [179264/267656]\n",
      "loss: 0.675920  [185664/267656]\n",
      "loss: 0.680888  [192064/267656]\n",
      "loss: 0.715010  [198464/267656]\n",
      "loss: 0.705279  [204864/267656]\n",
      "loss: 0.646117  [211264/267656]\n",
      "loss: 0.690828  [217664/267656]\n",
      "loss: 0.646020  [224064/267656]\n",
      "loss: 0.651263  [230464/267656]\n",
      "loss: 0.680889  [236864/267656]\n",
      "loss: 0.675899  [243264/267656]\n",
      "loss: 0.675836  [249664/267656]\n",
      "loss: 0.711188  [256064/267656]\n",
      "loss: 0.700946  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680468 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.690772  [   64/267656]\n",
      "loss: 0.700662  [ 6464/267656]\n",
      "loss: 0.680887  [12864/267656]\n",
      "loss: 0.700316  [19264/267656]\n",
      "loss: 0.671178  [25664/267656]\n",
      "loss: 0.671234  [32064/267656]\n",
      "loss: 0.666169  [38464/267656]\n",
      "loss: 0.690785  [44864/267656]\n",
      "loss: 0.680890  [51264/267656]\n",
      "loss: 0.700840  [57664/267656]\n",
      "loss: 0.635541  [64064/267656]\n",
      "loss: 0.706268  [70464/267656]\n",
      "loss: 0.680895  [76864/267656]\n",
      "loss: 0.645528  [83264/267656]\n",
      "loss: 0.685922  [89664/267656]\n",
      "loss: 0.690938  [96064/267656]\n",
      "loss: 0.660722  [102464/267656]\n",
      "loss: 0.660737  [108864/267656]\n",
      "loss: 0.706179  [115264/267656]\n",
      "loss: 0.675827  [121664/267656]\n",
      "loss: 0.680901  [128064/267656]\n",
      "loss: 0.650563  [134464/267656]\n",
      "loss: 0.670828  [140864/267656]\n",
      "loss: 0.701091  [147264/267656]\n",
      "loss: 0.665708  [153664/267656]\n",
      "loss: 0.680898  [160064/267656]\n",
      "loss: 0.716493  [166464/267656]\n",
      "loss: 0.730950  [172864/267656]\n",
      "loss: 0.660923  [179264/267656]\n",
      "loss: 0.685864  [185664/267656]\n",
      "loss: 0.680892  [192064/267656]\n",
      "loss: 0.670772  [198464/267656]\n",
      "loss: 0.675847  [204864/267656]\n",
      "loss: 0.675915  [211264/267656]\n",
      "loss: 0.666102  [217664/267656]\n",
      "loss: 0.700670  [224064/267656]\n",
      "loss: 0.690825  [230464/267656]\n",
      "loss: 0.690836  [236864/267656]\n",
      "loss: 0.670995  [243264/267656]\n",
      "loss: 0.671053  [249664/267656]\n",
      "loss: 0.685901  [256064/267656]\n",
      "loss: 0.685867  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680467 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.680887  [   64/267656]\n",
      "loss: 0.651063  [ 6464/267656]\n",
      "loss: 0.666259  [12864/267656]\n",
      "loss: 0.661219  [19264/267656]\n",
      "loss: 0.651076  [25664/267656]\n",
      "loss: 0.670859  [32064/267656]\n",
      "loss: 0.685853  [38464/267656]\n",
      "loss: 0.675959  [44864/267656]\n",
      "loss: 0.675944  [51264/267656]\n",
      "loss: 0.695768  [57664/267656]\n",
      "loss: 0.665981  [64064/267656]\n",
      "loss: 0.690937  [70464/267656]\n",
      "loss: 0.720901  [76864/267656]\n",
      "loss: 0.675977  [83264/267656]\n",
      "loss: 0.666172  [89664/267656]\n",
      "loss: 0.661118  [96064/267656]\n",
      "loss: 0.680889  [102464/267656]\n",
      "loss: 0.670926  [108864/267656]\n",
      "loss: 0.690816  [115264/267656]\n",
      "loss: 0.670941  [121664/267656]\n",
      "loss: 0.661006  [128064/267656]\n",
      "loss: 0.651073  [134464/267656]\n",
      "loss: 0.666061  [140864/267656]\n",
      "loss: 0.690882  [147264/267656]\n",
      "loss: 0.660691  [153664/267656]\n",
      "loss: 0.660542  [160064/267656]\n",
      "loss: 0.665619  [166464/267656]\n",
      "loss: 0.691187  [172864/267656]\n",
      "loss: 0.711569  [179264/267656]\n",
      "loss: 0.706310  [185664/267656]\n",
      "loss: 0.696109  [192064/267656]\n",
      "loss: 0.645520  [198464/267656]\n",
      "loss: 0.660756  [204864/267656]\n",
      "loss: 0.660879  [211264/267656]\n",
      "loss: 0.690888  [217664/267656]\n",
      "loss: 0.670824  [224064/267656]\n",
      "loss: 0.685889  [230464/267656]\n",
      "loss: 0.675975  [236864/267656]\n",
      "loss: 0.695581  [243264/267656]\n",
      "loss: 0.651212  [249664/267656]\n",
      "loss: 0.695769  [256064/267656]\n",
      "loss: 0.641122  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680449 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.675853  [   64/267656]\n",
      "loss: 0.685886  [ 6464/267656]\n",
      "loss: 0.685880  [12864/267656]\n",
      "loss: 0.695818  [19264/267656]\n",
      "loss: 0.666175  [25664/267656]\n",
      "loss: 0.685846  [32064/267656]\n",
      "loss: 0.685840  [38464/267656]\n",
      "loss: 0.685902  [44864/267656]\n",
      "loss: 0.680892  [51264/267656]\n",
      "loss: 0.695826  [57664/267656]\n",
      "loss: 0.666018  [64064/267656]\n",
      "loss: 0.680888  [70464/267656]\n",
      "loss: 0.685852  [76864/267656]\n",
      "loss: 0.700683  [83264/267656]\n",
      "loss: 0.715424  [89664/267656]\n",
      "loss: 0.680887  [96064/267656]\n",
      "loss: 0.671131  [102464/267656]\n",
      "loss: 0.695700  [108864/267656]\n",
      "loss: 0.715300  [115264/267656]\n",
      "loss: 0.700400  [121664/267656]\n",
      "loss: 0.661203  [128064/267656]\n",
      "loss: 0.680885  [134464/267656]\n",
      "loss: 0.675921  [140864/267656]\n",
      "loss: 0.656277  [147264/267656]\n",
      "loss: 0.695718  [153664/267656]\n",
      "loss: 0.661209  [160064/267656]\n",
      "loss: 0.734992  [166464/267656]\n",
      "loss: 0.690821  [172864/267656]\n",
      "loss: 0.680889  [179264/267656]\n",
      "loss: 0.685895  [185664/267656]\n",
      "loss: 0.650704  [192064/267656]\n",
      "loss: 0.665534  [198464/267656]\n",
      "loss: 0.696084  [204864/267656]\n",
      "loss: 0.706215  [211264/267656]\n",
      "loss: 0.680896  [217664/267656]\n",
      "loss: 0.670708  [224064/267656]\n",
      "loss: 0.685992  [230464/267656]\n",
      "loss: 0.675798  [236864/267656]\n",
      "loss: 0.680899  [243264/267656]\n",
      "loss: 0.731738  [249664/267656]\n",
      "loss: 0.655326  [256064/267656]\n",
      "loss: 0.660612  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680471 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.655343  [   64/267656]\n",
      "loss: 0.670688  [ 6464/267656]\n",
      "loss: 0.665600  [12864/267656]\n",
      "loss: 0.701192  [19264/267656]\n",
      "loss: 0.716082  [25664/267656]\n",
      "loss: 0.685933  [32064/267656]\n",
      "loss: 0.706166  [38464/267656]\n",
      "loss: 0.696056  [44864/267656]\n",
      "loss: 0.675876  [51264/267656]\n",
      "loss: 0.680886  [57664/267656]\n",
      "loss: 0.685820  [64064/267656]\n",
      "loss: 0.720802  [70464/267656]\n",
      "loss: 0.690970  [76864/267656]\n",
      "loss: 0.670710  [83264/267656]\n",
      "loss: 0.675814  [89664/267656]\n",
      "loss: 0.696276  [96064/267656]\n",
      "loss: 0.686083  [102464/267656]\n",
      "loss: 0.655439  [108864/267656]\n",
      "loss: 0.675792  [115264/267656]\n",
      "loss: 0.711988  [121664/267656]\n",
      "loss: 0.665545  [128064/267656]\n",
      "loss: 0.675820  [134464/267656]\n",
      "loss: 0.670811  [140864/267656]\n",
      "loss: 0.701188  [147264/267656]\n",
      "loss: 0.706089  [153664/267656]\n",
      "loss: 0.695928  [160064/267656]\n",
      "loss: 0.690871  [166464/267656]\n",
      "loss: 0.665736  [172864/267656]\n",
      "loss: 0.685975  [179264/267656]\n",
      "loss: 0.691027  [185664/267656]\n",
      "loss: 0.691058  [192064/267656]\n",
      "loss: 0.675849  [198464/267656]\n",
      "loss: 0.690961  [204864/267656]\n",
      "loss: 0.675865  [211264/267656]\n",
      "loss: 0.670858  [217664/267656]\n",
      "loss: 0.675875  [224064/267656]\n",
      "loss: 0.665937  [230464/267656]\n",
      "loss: 0.680891  [236864/267656]\n",
      "loss: 0.690886  [243264/267656]\n",
      "loss: 0.666182  [249664/267656]\n",
      "loss: 0.651590  [256064/267656]\n",
      "loss: 0.715543  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680463 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.666048  [   64/267656]\n",
      "loss: 0.670859  [ 6464/267656]\n",
      "loss: 0.685965  [12864/267656]\n",
      "loss: 0.685935  [19264/267656]\n",
      "loss: 0.721248  [25664/267656]\n",
      "loss: 0.685887  [32064/267656]\n",
      "loss: 0.685906  [38464/267656]\n",
      "loss: 0.685943  [44864/267656]\n",
      "loss: 0.670708  [51264/267656]\n",
      "loss: 0.711342  [57664/267656]\n",
      "loss: 0.680895  [64064/267656]\n",
      "loss: 0.680898  [70464/267656]\n",
      "loss: 0.680892  [76864/267656]\n",
      "loss: 0.675804  [83264/267656]\n",
      "loss: 0.680893  [89664/267656]\n",
      "loss: 0.685920  [96064/267656]\n",
      "loss: 0.665612  [102464/267656]\n",
      "loss: 0.680899  [108864/267656]\n",
      "loss: 0.691039  [115264/267656]\n",
      "loss: 0.670851  [121664/267656]\n",
      "loss: 0.680894  [128064/267656]\n",
      "loss: 0.675837  [134464/267656]\n",
      "loss: 0.680901  [140864/267656]\n",
      "loss: 0.655751  [147264/267656]\n",
      "loss: 0.670832  [153664/267656]\n",
      "loss: 0.675847  [160064/267656]\n",
      "loss: 0.695927  [166464/267656]\n",
      "loss: 0.660807  [172864/267656]\n",
      "loss: 0.695862  [179264/267656]\n",
      "loss: 0.705936  [185664/267656]\n",
      "loss: 0.675830  [192064/267656]\n",
      "loss: 0.710863  [198464/267656]\n",
      "loss: 0.720884  [204864/267656]\n",
      "loss: 0.695879  [211264/267656]\n",
      "loss: 0.695854  [217664/267656]\n",
      "loss: 0.695956  [224064/267656]\n",
      "loss: 0.710664  [230464/267656]\n",
      "loss: 0.705860  [236864/267656]\n",
      "loss: 0.695454  [243264/267656]\n",
      "loss: 0.671104  [249664/267656]\n",
      "loss: 0.680888  [256064/267656]\n",
      "loss: 0.710350  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680467 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.695613  [   64/267656]\n",
      "loss: 0.700818  [ 6464/267656]\n",
      "loss: 0.715579  [12864/267656]\n",
      "loss: 0.685871  [19264/267656]\n",
      "loss: 0.715586  [25664/267656]\n",
      "loss: 0.700486  [32064/267656]\n",
      "loss: 0.656815  [38464/267656]\n",
      "loss: 0.690507  [44864/267656]\n",
      "loss: 0.666474  [51264/267656]\n",
      "loss: 0.661421  [57664/267656]\n",
      "loss: 0.680888  [64064/267656]\n",
      "loss: 0.700863  [70464/267656]\n",
      "loss: 0.665830  [76864/267656]\n",
      "loss: 0.665774  [83264/267656]\n",
      "loss: 0.696218  [89664/267656]\n",
      "loss: 0.645443  [96064/267656]\n",
      "loss: 0.696196  [102464/267656]\n",
      "loss: 0.686020  [108864/267656]\n",
      "loss: 0.716999  [115264/267656]\n",
      "loss: 0.665526  [121664/267656]\n",
      "loss: 0.685932  [128064/267656]\n",
      "loss: 0.685869  [134464/267656]\n",
      "loss: 0.636023  [140864/267656]\n",
      "loss: 0.666129  [147264/267656]\n",
      "loss: 0.671106  [153664/267656]\n",
      "loss: 0.671141  [160064/267656]\n",
      "loss: 0.685727  [166464/267656]\n",
      "loss: 0.671124  [172864/267656]\n",
      "loss: 0.710419  [179264/267656]\n",
      "loss: 0.680885  [185664/267656]\n",
      "loss: 0.700797  [192064/267656]\n",
      "loss: 0.665906  [198464/267656]\n",
      "loss: 0.646143  [204864/267656]\n",
      "loss: 0.660857  [211264/267656]\n",
      "loss: 0.685931  [217664/267656]\n",
      "loss: 0.691010  [224064/267656]\n",
      "loss: 0.691058  [230464/267656]\n",
      "loss: 0.675835  [236864/267656]\n",
      "loss: 0.685966  [243264/267656]\n",
      "loss: 0.705995  [249664/267656]\n",
      "loss: 0.675845  [256064/267656]\n",
      "loss: 0.645579  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680470 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.690933  [   64/267656]\n",
      "loss: 0.656060  [ 6464/267656]\n",
      "loss: 0.656238  [12864/267656]\n",
      "loss: 0.715140  [19264/267656]\n",
      "loss: 0.690791  [25664/267656]\n",
      "loss: 0.675893  [32064/267656]\n",
      "loss: 0.660575  [38464/267656]\n",
      "loss: 0.665608  [44864/267656]\n",
      "loss: 0.670722  [51264/267656]\n",
      "loss: 0.675838  [57664/267656]\n",
      "loss: 0.661024  [64064/267656]\n",
      "loss: 0.680888  [70464/267656]\n",
      "loss: 0.665862  [76864/267656]\n",
      "loss: 0.680895  [83264/267656]\n",
      "loss: 0.655627  [89664/267656]\n",
      "loss: 0.675873  [96064/267656]\n",
      "loss: 0.665709  [102464/267656]\n",
      "loss: 0.680902  [108864/267656]\n",
      "loss: 0.726678  [115264/267656]\n",
      "loss: 0.680899  [121664/267656]\n",
      "loss: 0.701402  [128064/267656]\n",
      "loss: 0.675804  [134464/267656]\n",
      "loss: 0.711309  [140864/267656]\n",
      "loss: 0.691096  [147264/267656]\n",
      "loss: 0.680902  [153664/267656]\n",
      "loss: 0.635308  [160064/267656]\n",
      "loss: 0.701034  [166464/267656]\n",
      "loss: 0.701034  [172864/267656]\n",
      "loss: 0.701183  [179264/267656]\n",
      "loss: 0.665891  [185664/267656]\n",
      "loss: 0.665733  [192064/267656]\n",
      "loss: 0.660582  [198464/267656]\n",
      "loss: 0.706314  [204864/267656]\n",
      "loss: 0.696146  [211264/267656]\n",
      "loss: 0.690889  [217664/267656]\n",
      "loss: 0.675932  [224064/267656]\n",
      "loss: 0.705810  [230464/267656]\n",
      "loss: 0.680888  [236864/267656]\n",
      "loss: 0.670995  [243264/267656]\n",
      "loss: 0.695724  [249664/267656]\n",
      "loss: 0.665932  [256064/267656]\n",
      "loss: 0.685860  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680467 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.675979  [   64/267656]\n",
      "loss: 0.675958  [ 6464/267656]\n",
      "loss: 0.695580  [12864/267656]\n",
      "loss: 0.680888  [19264/267656]\n",
      "loss: 0.725252  [25664/267656]\n",
      "loss: 0.680886  [32064/267656]\n",
      "loss: 0.671013  [38464/267656]\n",
      "loss: 0.690827  [44864/267656]\n",
      "loss: 0.695800  [51264/267656]\n",
      "loss: 0.680888  [57664/267656]\n",
      "loss: 0.646341  [64064/267656]\n",
      "loss: 0.670994  [70464/267656]\n",
      "loss: 0.671066  [76864/267656]\n",
      "loss: 0.690633  [83264/267656]\n",
      "loss: 0.680887  [89664/267656]\n",
      "loss: 0.705572  [96064/267656]\n",
      "loss: 0.690746  [102464/267656]\n",
      "loss: 0.715320  [108864/267656]\n",
      "loss: 0.675950  [115264/267656]\n",
      "loss: 0.641654  [121664/267656]\n",
      "loss: 0.666121  [128064/267656]\n",
      "loss: 0.705675  [134464/267656]\n",
      "loss: 0.685922  [140864/267656]\n",
      "loss: 0.690822  [147264/267656]\n",
      "loss: 0.661062  [153664/267656]\n",
      "loss: 0.646281  [160064/267656]\n",
      "loss: 0.646005  [166464/267656]\n",
      "loss: 0.660433  [172864/267656]\n",
      "loss: 0.696323  [179264/267656]\n",
      "loss: 0.680904  [185664/267656]\n",
      "loss: 0.696226  [192064/267656]\n",
      "loss: 0.691145  [198464/267656]\n",
      "loss: 0.660446  [204864/267656]\n",
      "loss: 0.675785  [211264/267656]\n",
      "loss: 0.650083  [217664/267656]\n",
      "loss: 0.670659  [224064/267656]\n",
      "loss: 0.680900  [230464/267656]\n",
      "loss: 0.701031  [236864/267656]\n",
      "loss: 0.701116  [243264/267656]\n",
      "loss: 0.696132  [249664/267656]\n",
      "loss: 0.670874  [256064/267656]\n",
      "loss: 0.670986  [262464/267656]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680462 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "loss_hist = []\n",
    "accuracy_hist = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, batch_size)\n",
    "    loss_hist.append(loss_fn(model(xtrain), ytrain).item())\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    accuracy_hist.append((model(xtest).argmax(1) == ytest).type(torch.float).sum().item()/len(ytest))\n",
    "    torch.save(model.state_dict(), f'./results/NN/save_parameters/model_lags{lags}_ntraps{3}_epoch{t}.pth')\n",
    "    \n",
    "    \n",
    "print(\"Done!\")\n",
    "utils.play_ending_song()\n",
    "utils.stop_ending_song()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.42055474190752307)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ovitraps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
