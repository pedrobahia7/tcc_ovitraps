{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.10)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\UFMG\\Periodos\\Periodo_XIV\\TCC\\Projeto\\CÃ³digos\\tcc_ovitraps\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, targets,model_type):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        if model_type == 'classifier':\n",
    "            self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        elif model_type == 'regressor':\n",
    "            self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "def scale_column(x_train:pd.DataFrame, x_test:pd.DataFrame, column:list):\n",
    "    \"\"\"\n",
    "    Move to utils_NN.py\n",
    "    Scales the same nature columns of x_train and x_test using the MinMaxScaler. The reference column is the one with the maximum value.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    max_value = x_train[column].max().max()\n",
    "    x_train[column] = x_train[column]/max_value\n",
    "    x_test[column]  = x_test[column]/max_value\n",
    "    return x_train, x_test, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([ovos_flag.rename('ovos_flag', inplace=True),data['novos']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "model_type =  'regressor' # 'classifier' or 'regressor'\n",
    "use_trap_info = True\n",
    "ntraps = 3\n",
    "lags = 3\n",
    "random_split = True\n",
    "test_size = 0.2\n",
    "scale = False\n",
    "learning_rate =1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'model_type': model_type,\n",
    "'use_trap_info': use_trap_info,\n",
    "'ntraps': ntraps,\n",
    "'lags': lags,\n",
    "'random_split': random_split,\n",
    "'test_size': test_size,\n",
    "'scale': scale,\n",
    "'learning_rate': learning_rate,\n",
    "'batch_size': batch_size,\n",
    "'epochs': epochs   \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import and preprocessing\n",
    "data = pd.read_csv(f'./results/final_df_lag{lags}_ntraps{ntraps}.csv')\n",
    "n = data.shape[0]\n",
    "nplaca_index = data['nplaca']\n",
    "data.drop(columns=['nplaca','distance0'], inplace=True) # drop distance0 because it is always zero\n",
    "ovos_flag = data['novos'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# divide columns into groups\n",
    "days_columns = [f'days{i}_lag{j}' for i in range(ntraps) for j in range(1, lags+1)]\n",
    "distance_columns = [f'distance{i}' for i in range(1,ntraps)]\n",
    "eggs_columns = [f'trap{i}_lag{j}' for i in range(ntraps) for j in range(1, lags+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of x and y\n",
    "if model_type == 'classifier':\n",
    "    y = ovos_flag\n",
    "elif model_type == 'regressor':\n",
    "    y = data['novos']\n",
    "\n",
    "if use_trap_info:\n",
    "    x = data.drop(columns=['novos'])\n",
    "else:\n",
    "    drop_cols = ['novos'] + days_columns + distance_columns\n",
    "    x = data.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = 1 - test_size\n",
    "\n",
    "if random_split:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42, stratify=ovos_flag)\n",
    "else:\n",
    "    y_train = y.iloc[:int(n*train_size)]\n",
    "    y_test = y.iloc[int(n*train_size):]\n",
    "    x_train = x.iloc[:int(n*train_size)]\n",
    "    x_test = x.iloc[int(n*train_size):]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "if scale:\n",
    "    x_train, x_test, max_eggs = scale_column(x_train, x_test, eggs_columns)\n",
    "    if use_trap_info:\n",
    "        x_train, x_test, max_distance = scale_column(x_train, x_test, distance_columns)\n",
    "        x_train, x_test, max_days = scale_column(x_train, x_test, days_columns)\n",
    "\n",
    "\n",
    "    if model_type != 'classifier':\n",
    "        y_train = y_train/max_eggs\n",
    "        y_test = y_test/max_eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensors\n",
    "xtrain = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "xtest = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "if model_type == 'classifier':\n",
    "    ytrain = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "    ytest = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "elif model_type == 'regressor':\n",
    "    ytrain = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    ytest = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "train_dataset = CustomDataset(xtrain, ytrain,model_type)\n",
    "test_dataset = CustomDataset(xtest, ytest,model_type)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=random_split)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=random_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network structure\n",
    "if use_trap_info:\n",
    "    model_input = lags*ntraps + ntraps-1 + ntraps*lags # sum  of eggs, distances minus one and days\n",
    "else:\n",
    "    model_input = lags*ntraps\n",
    "    \n",
    "if model_type == 'classifier':\n",
    "    model_output = 2\n",
    "elif model_type == 'regressor':\n",
    "    model_output = 1\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.structure = nn.Sequential(\n",
    "            nn.Linear(model_input, 20),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Linear(10, 5),\n",
    "            nn.Linear(5, model_output)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.structure(x)\n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test loops\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,batch_size):\n",
    "    size = xtrain.shape[0]\n",
    "    model.train()\n",
    "    for batch, (xtest, ytest) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xtest)\n",
    "        loss = loss_fn(pred, ytest)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(xtest)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn,model_type):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, acc = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            if model_type == 'classifier':\n",
    "                pred = model(X)\n",
    "                acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            elif model_type == 'regressor':\n",
    "                pred = torch.round(model(X))\n",
    "                acc += ((pred.round() == y).type(torch.float)).sum().item()\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    acc /= size\n",
    "    print(f\"Test Metrics: \\n Accuracy: {(100*acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "if model_type == 'classifier':\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "elif model_type == 'regressor':\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "loss_hist = []\n",
    "accuracy_hist = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, batch_size)\n",
    "    acc, test_loss = test_loop(test_dataloader, model, loss_fn,model_type)\n",
    "    \n",
    "    accuracy_hist.append(acc)\n",
    "    loss_hist.append(test_loss)\n",
    "    torch.save(model.state_dict(), f'./results/NN/save_parameters/model{model_type}_lags{lags}_ntraps{3}_epoch{t}.pth')\n",
    "    \n",
    "    \n",
    "print(\"Done!\")\n",
    "utils.play_ending_song()\n",
    "utils.stop_ending_song(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'classifier':\n",
    "    yhat = model(xtest).argmax(1).cpu().numpy()\n",
    "elif model_type == 'regressor':\n",
    "    yhat = model(xtest).round().cpu().detach().numpy() \n",
    "\n",
    "if model_type == 'classifier':\n",
    "    print(accuracy_score(y_test, yhat))\n",
    "    print(confusion_matrix(y_test, yhat, normalize='true', labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_path = f'./results/NN/save_structure/model{model_type}_lags{lags}_ntraps{3}_structure.pth'\n",
    "torch.save(model, structure_path)\n",
    "\n",
    "new_results = {\n",
    "    'model': model_type,\n",
    "    'net_structure': structure_path,\n",
    "    'repetition': 1,\n",
    "    'parameters': parameters,\n",
    "    'date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'accuracy': accuracy_hist,\n",
    "    'loss': loss_hist,\n",
    "    'yhat': yhat.tolist(),\n",
    "    'ytest': ytest.cpu().numpy().tolist(),\n",
    "}\n",
    "\n",
    "\n",
    "# Load the existing JSON file\n",
    "with open('./results/NN/model_accuracies.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "    # Update with new results\n",
    "\n",
    "\n",
    "for item in results:\n",
    "    if item['parameters'] == new_results['parameters'] and item['net_structure'] == new_results['net_structure']:\n",
    "        new_results['repetition'] = item['repetition'] + 1\n",
    "results.append(new_results)\n",
    "\n",
    "\n",
    "\n",
    "with open('./results/NN/model_accuracies.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the existing JSON file\n",
    "with open('./results/NN/model_accuracies.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "    # Update with new results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xtest).round() == ytest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_triad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
